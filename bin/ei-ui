#!/usr/bin/python
from __future__ import annotations
import os

# from typing import Text, Dict, Optional
# from tempfile import TemporaryDirectory
import pandas as pd


# Streamlit imports
import streamlit as st
import streamlit.components.v1 as components

# from annotated_text import annotated_text
from audio_recorder_streamlit import audio_recorder


# imports
from ei.asr import WhisperModel, normalize, read_data
from ei.utils import word_level_edit_distance, visualize_matrix


def streamlit_ui():
    components.html(
        """
                <div style="background-color:#475e5f;padding:10px;">
                <h2 style="color:white;text-align:center;font-size:30px">Elicited Imitation</h2>
                <h4 style="color:white;text-align:center; font-size:20px;">Testing Suite</h4>
                </div>
                """
    )
    if "load_models" not in st.session_state:
        st.session_state.load_models = False
    # # (1) load whisper (2) load translation
    if st.session_state.load_models == False:
        st.subheader("Whisper ASR Model")
        with st.spinner("Loading Whisper ASR model ..."):
            speech_model = WhisperModel(model_name="openai/whisper-tiny", language="en")
            st.success("Whisper ASR model loaded successfully")

    # data = pd.read_csv('sample/data.csv')
    dataset = pd.read_csv(os.path.join("english", "data.csv"))
    dataset["path"] = "english" + "/" + dataset["audio"]
    # dataset = datasets.Dataset.from_pandas(dataset)

    # State management for Streamlit
    if "index" not in st.session_state:
        st.session_state.index = 0

    st.subheader("1. Listen To Audio Carefully")
    # Display audio and transcript
    audio_file = dataset.iloc[st.session_state.index]["path"]
    gold = dataset.iloc[st.session_state.index]["transcript"]

    st.audio(audio_file)
    st.write("FOR DEBUGGING==>", gold)

    st.subheader("2. Record Your Speech")

    audio_bytes = audio_recorder(pause_threshold=2.0)
    if audio_bytes:
        st.subheader("3. Your Speech! - FOR DEBUGGING")
        st.audio(audio_bytes, format="audio/wav")
        # FIXME: Can we depend on writing to container in deployment?
        with open("WAVE_FILE.wav", "wb") as f:
            f.write(audio_bytes)

        st.success("Audio recorded successfully")

        audio_file = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "WAVE_FILE.wav"
        )
        audio_file = "./WAVE_FILE.wav"
        st.subheader("4. Your Speech Transcribed")
        with st.spinner("Please wait ... generating the transcription "):
            transcript = speech_model.transcribe(audio_file)
        st.write(transcript)

        res = word_level_edit_distance(transcript, gold)
        st.write(res)

    # Next button to advance to the next audio/transcript
    if st.button("Next Audio"):
        st.session_state.index = (st.session_state.index + 1) % len(dataset)


if __name__ == "__main__":
    streamlit_ui()
