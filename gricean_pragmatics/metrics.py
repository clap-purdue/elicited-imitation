# -*- coding: utf-8 -*-
from typing import List, Dict
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


__all__  = ["GPMetrics"]


class GPMetrics:
    
    @staticmethod
    def calculate_naturalness(utterance_surprisal: float, interpretation_a_surprisal: float, interpretation_b_surprisal: float) -> Dict:
        """
        Calculate the naturalness metric based on surprisal scores.

        Parameters:
        utterance_surprisal (float): Surprisal score for the utterance.
        interpretation_a_surprisal (float): Surprisal score for interpretation (a).
        interpretation_b_surprisal (float): Surprisal score for interpretation (b).

        Returns:
        dict: A dictionary with surprisal differences.
        """
        difference_a = abs(utterance_surprisal - interpretation_a_surprisal)
        difference_b = abs(utterance_surprisal - interpretation_b_surprisal)
        
        return {
            "difference_a": difference_a,
            "difference_b": difference_b,
            "preferred_interpretation": "b" if difference_b < difference_a else "a"
        }

    @staticmethod
    def calculate_ssm(embedding_a: np.ndarray, embedding_b: np.ndarray, embedding_c: np.ndarray) -> dict:
        """
        Calculate the Sensitivity to different Shades of Meaning (SSM) metric using cosine similarity.

        Parameters:
        embedding_a (np.ndarray): Embedding for the first sentence (e.g., "Alex was not unaware of the issue").
        embedding_b (np.ndarray): Embedding for the second sentence (e.g., "Alex was slightly aware of the issue").
        embedding_c (np.ndarray): Embedding for the third sentence (e.g., "Alex was aware of the issue").

        Returns:
        dict: A dictionary with similarity scores and their ranking.
        """
        # Calculate cosine similarities
        similarity_ab = cosine_similarity([embedding_a], [embedding_b])[0][0]
        similarity_bc = cosine_similarity([embedding_b], [embedding_c])[0][0]
        
        # Calculate the ranking
        ranking = sorted(
            [("similarity_ab", similarity_ab), ("similarity_bc", similarity_bc)], 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return {
            "similarity_ab": similarity_ab,
            "similarity_bc": similarity_bc,
            "ranking": ranking
        }


    @staticmethod
    def evaluate_prc(llm_responses: List[str], expected_steps: List[str]) -> Dict[str, float]:
        """
        Evaluate the Pragmatic Reasoning Chains (PRC) metric by comparing the LLM's reasoning steps
        to the expected reasoning steps.

        Parameters:
        llm_responses (List[str]): The reasoning steps generated by the LLM.
        expected_steps (List[str]): The correct reasoning steps based on formal pragmatics.

        Returns:
        Dict[str, float]: A dictionary containing the accuracy and step-by-step alignment score.
        """
        correct_steps = 0
        step_scores = []
        
        # Compare each LLM response with the corresponding expected step
        for llm_step, expected_step in zip(llm_responses, expected_steps):
            if llm_step.strip().lower() == expected_step.strip().lower():
                correct_steps += 1
                step_scores.append(1.0)
            else:
                step_scores.append(0.0)
        
        # Calculate overall accuracy
        accuracy = correct_steps / len(expected_steps)
        
        return {
            "accuracy": accuracy,
            "step_scores": step_scores
        }

    @staticmethod
    def calculate_irr(successful_recoveries: int, total_errors: int) -> float:
        """
        Calculate the Implicature Recovery Rate (IRR) metric.

        Parameters:
        successful_recoveries (int): The number of successfully recovered implicatures.
        total_errors (int): The total number of implicature errors introduced.

        Returns:
        float: The Implicature Recovery Rate (IRR).
        """
        if total_errors == 0:
            return 0.0
        
        irr = successful_recoveries / total_errors
        return irr


    @staticmethod
    def calculate_psi(original_accuracy: float, changed_accuracy: float) -> float:
        """
        Calculate the Pragmatic Sensitivity Index (PSI) metric.

        Parameters:
        original_accuracy (float): The accuracy of the LLM's responses in the original context.
        changed_accuracy (float): The accuracy of the LLM's responses after subtle contextual changes.

        Returns:
        float: The Pragmatic Sensitivity Index (PSI).
        """
        psi = original_accuracy - changed_accuracy
        return psi

